<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>testing on Jack Baker</title>
    <link>http://jackcbaker.github.io/tags/testing/</link>
    <description>Recent content in testing on Jack Baker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 11 Apr 2021 15:36:34 +0100</lastBuildDate><atom:link href="http://jackcbaker.github.io/tags/testing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to tell your data science pipeline is actually working?</title>
      <link>http://jackcbaker.github.io/posts/check-data-science-pipeline-working/</link>
      <pubDate>Sun, 11 Apr 2021 15:36:34 +0100</pubDate>
      
      <guid>http://jackcbaker.github.io/posts/check-data-science-pipeline-working/</guid>
      <description>I&amp;rsquo;ve been there - studying model output for hours to check a model is working. There&amp;rsquo;s some better ways than this. For example designing backtests to check a model does better than a baseline. The trouble with these methods is they don&amp;rsquo;t categorically show that a model doing exactly as it is designed to do; they&amp;rsquo;re also not automated, or (as with a backtest) are expensive to run; and sometimes they&amp;rsquo;re harder to design than the model itself!</description>
    </item>
    
  </channel>
</rss>
