<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jack Baker</title>
    <link>https://jackbakerds.com/</link>
    <description>Recent content on Jack Baker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 06 Jun 2021 15:59:00 +0100</lastBuildDate><atom:link href="https://jackbakerds.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Upweighting your recent observations in regression and classification</title>
      <link>https://jackbakerds.com/posts/upweight-recent-observations-regression-classification/</link>
      <pubDate>Sun, 06 Jun 2021 15:59:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/upweight-recent-observations-regression-classification/</guid>
      <description>&lt;p&gt;In any regression or classification problem where observations have a time element, old patterns can become stale. For this reason, I&amp;rsquo;m often asking myself &amp;ndash; how do I upweight my most recent observations? In this post I explain how to do this.&lt;/p&gt;
&lt;p&gt;All the code in this tutorial is available as a &lt;a href=&#34;https://github.com/jackcbaker/blog-notebooks/blob/main/regression-forgetting.ipynb&#34;&gt;jupyter notebook&lt;/a&gt; on my &lt;a href=&#34;https://github.com/jackcbaker/&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-1-fetch-the-data&#34;&gt;
  Step 1: Fetch the data
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-1-fetch-the-data&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;For this post I&amp;rsquo;ll be using price data from the &lt;a href=&#34;https://www.kaggle.com/camnugent/sandp500&#34;&gt;S&amp;amp;P500 available via Kaggle&amp;rsquo;s&lt;/a&gt; great dataset library. This dataset is released under the &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;creative commons 0 license&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A common problem in financial investments is to ensure you have a &lt;em&gt;balanced portfolio&lt;/em&gt;. If your portfolio is unbalanced it means that if one stock starts to perform poorly, all of them do. This means any portfolio risks can have a massive effect on value.&lt;/p&gt;
&lt;p&gt;The ideal scenario is that if any stock started to decrease, this would be &lt;em&gt;balanced&lt;/em&gt; by an increase in another one: a balanced portfolio.&lt;/p&gt;
&lt;p&gt;So we can just calculate the correlation coefficient between all the stocks in our portfolio and be done right? Not quite. These relationships tend to change through time, which is something we need to account for.&lt;/p&gt;
&lt;p&gt;In this post we&amp;rsquo;ll use a regression model where recent observations are given more weight to model the relationship between two stocks. This ensures when deciding how balanced the stocks are we are taking into account that the relationship is likely to change over time.&lt;/p&gt;
&lt;p&gt;The tutorial can be applied to any regression or classification problem where you suspect observations are likely to &amp;lsquo;get stale&amp;rsquo; or where relationships may change over time.&lt;/p&gt;
&lt;p&gt;For this tutorial we&amp;rsquo;re going to compare the closing share price of American Express (AXP) to Apple&amp;rsquo;s (AAPL). First let&amp;rsquo;s load the dataset after downloading it from Kaggle&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#75715e&#34;&gt;# for plotting&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns
&lt;span style=&#34;color:#75715e&#34;&gt;# We&amp;#39;ll need this later...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LinearRegression

&lt;span style=&#34;color:#75715e&#34;&gt;# Load in the full dataset rather than individual stocks&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/all_stocks_5yr.csv&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Subset to just Nike and Apple stocks&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isin([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;])]
&lt;span style=&#34;color:#75715e&#34;&gt;# Just take the columns we need&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;]]
&lt;span style=&#34;color:#75715e&#34;&gt;# Set dates to be datetime objects&lt;/span&gt;
snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_datetime(snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])
&lt;span style=&#34;color:#75715e&#34;&gt;# Plot the data&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lineplot(data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df, x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;, hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Reshape the data to wide format&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pivot(index&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;, values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/upweight-recent-observations/stock_plot.png&#34; alt=&#34;Plot of the closing prices of Apple stock vs. American Express&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Plot of the closing prices of Apple vs. American Express.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;We can see from the plot above that initially there is quite a strong relationship between Apple and American Express prices. This is weakened when the American Express price starts to fall. But after this the relationship appears to get stronger again. This suggests a model that accounts for the fact that relationships change through time might be better. It also suggests that most of the time these stocks are imbalanced: they tend to move together.&lt;/p&gt;
&lt;h2 id=&#34;step-2-adding-weights&#34;&gt;
  Step 2: Adding weights
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-2-adding-weights&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Now we need to decide how to weight the observations. There are lots of options here, which is a bit confusing.&lt;/p&gt;
&lt;p&gt;I tend to use the same weights as used in &lt;a href=&#34;https://otexts.com/fpp3/ses.html&#34;&gt;exponential smoothing models&lt;/a&gt;. Why? Exponential smoothing is a popular, simple forecasting method that has been around for over 60 years! It&amp;rsquo;s tried and tested, and has not gone away.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s set \(T\) to be the time of the most recent observation, \(t\) to be the time of the observation we&amp;rsquo;re interested in, and \(\gamma\) to be a hyperparameter we pick that&amp;rsquo;s between 0 and 1. Then I set my weights \(w\) to be&lt;/p&gt;
&lt;p&gt;\begin{equation}
w = \gamma^{[T-t]}.
\end{equation}&lt;/p&gt;
&lt;p&gt;What does this mean? Suppose we set \(\gamma = 0.95\). Then if my observation is made at the most recent time point, its weight will be 1. If it&amp;rsquo;s made at the second most recent time point, its weight will be 0.95. If it&amp;rsquo;s made at the 10th most recent time point, its weight will be \(\gamma^{10} = 0.95^{10} \approx 0.6\). Essentially our weight smoothly decreases to nothing as the observation gets older and older.&lt;/p&gt;
&lt;p&gt;An unfortunate side effect to this is we&amp;rsquo;ve added a hyperparameter to tune. I often just quickly do this by eye looking at the data, but you can also tune this in the normal way using cross-validation or AIC/BIC. For this tutorial, I&amp;rsquo;ll just set it to 0.8.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s add weights to our data now:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set hyperparam&lt;/span&gt;
gamma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Check both stocks go up to the same date&lt;/span&gt;
most_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max()
days_before_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (most_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days
snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gamma &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; days_before_recent_date&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values
snp_df
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;AAPL&lt;/th&gt;
      &lt;th&gt;AXP&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;67.8542&lt;/td&gt;
      &lt;td&gt;61.80&lt;/td&gt;
      &lt;td&gt;1.377927e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-02-11&lt;/td&gt;
      &lt;td&gt;68.5614&lt;/td&gt;
      &lt;td&gt;61.98&lt;/td&gt;
      &lt;td&gt;2.691264e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2013-02-12&lt;/td&gt;
      &lt;td&gt;66.8428&lt;/td&gt;
      &lt;td&gt;62.20&lt;/td&gt;
      &lt;td&gt;3.364080e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-02-13&lt;/td&gt;
      &lt;td&gt;66.7156&lt;/td&gt;
      &lt;td&gt;62.10&lt;/td&gt;
      &lt;td&gt;4.205100e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2013-02-14&lt;/td&gt;
      &lt;td&gt;66.6556&lt;/td&gt;
      &lt;td&gt;62.34&lt;/td&gt;
      &lt;td&gt;5.256375e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1254&lt;/th&gt;
      &lt;td&gt;2018-02-01&lt;/td&gt;
      &lt;td&gt;167.7800&lt;/td&gt;
      &lt;td&gt;100.00&lt;/td&gt;
      &lt;td&gt;2.621440e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1255&lt;/th&gt;
      &lt;td&gt;2018-02-02&lt;/td&gt;
      &lt;td&gt;160.5000&lt;/td&gt;
      &lt;td&gt;96.68&lt;/td&gt;
      &lt;td&gt;3.276800e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1256&lt;/th&gt;
      &lt;td&gt;2018-02-05&lt;/td&gt;
      &lt;td&gt;156.4900&lt;/td&gt;
      &lt;td&gt;92.01&lt;/td&gt;
      &lt;td&gt;6.400000e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1257&lt;/th&gt;
      &lt;td&gt;2018-02-06&lt;/td&gt;
      &lt;td&gt;163.0300&lt;/td&gt;
      &lt;td&gt;94.18&lt;/td&gt;
      &lt;td&gt;8.000000e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1258&lt;/th&gt;
      &lt;td&gt;2018-02-07&lt;/td&gt;
      &lt;td&gt;159.5400&lt;/td&gt;
      &lt;td&gt;93.61&lt;/td&gt;
      &lt;td&gt;1.000000e+00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1259 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;step-3-fit-your-model&#34;&gt;
  Step 3: Fit your model
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-3-fit-your-model&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;You might think that fitting our model will become a challenge now we&amp;rsquo;ve added weights. It&amp;rsquo;s actually dead easy.&lt;/p&gt;
&lt;p&gt;Most regression and classification algorithms allow you to provide a dataset weight: for tree based methods (sklearn random forest, xgboost, lightgbm), you just set the &lt;code&gt;sample_weight&lt;/code&gt; in the &lt;code&gt;fit&lt;/code&gt; function; for linear regression R&amp;rsquo;s &lt;code&gt;lm&lt;/code&gt; function has a &lt;code&gt;weights&lt;/code&gt; argument, sklearn&amp;rsquo;s &lt;code&gt;LinearRegression&lt;/code&gt; has a &lt;code&gt;sample_weight&lt;/code&gt; argument in the &lt;code&gt;fit&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If your algorithm does not allow you to set a weight, you can borrow from the class imbalance techniques and &lt;a href=&#34;https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/&#34;&gt;oversample/undersample&lt;/a&gt; your observations based on your weights.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit our model using sklearn&amp;rsquo;s linear regression:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;weighted_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LinearRegression()
weighted_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weighted_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;], 
    sample_weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#75715e&#34;&gt;# Return the R^2 score for our model&lt;/span&gt;
r2_weighted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weighted_results&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;], 
    sample_weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;R^2 score for the weighted model is {r2_weighted}&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Linear coefficient of relationship of Apple to American Express is {weighted_model.coef_[0]}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;R^2 score for the weighted model is 0.740848229773393
Linear coefficient of relationship of Apple to American Express is 0.4810444526330664
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare this to an unweighted model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;unweighted_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LinearRegression()
unweighted_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; unweighted_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#75715e&#34;&gt;# Return the R^2 score for our model&lt;/span&gt;
r2_unweighted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; unweighted_results&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;R^2 score for the unweighted model is {r2_unweighted}&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Linear coefficient of relationship of Apple to American Express is {unweighted_model.coef_[0]}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;R^2 score for the unweighted model is 0.1161401914047755
Linear coefficient of relationship of Apple to American Express is 0.12149913600503894
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we&amp;rsquo;ve improved our in-sample fit significantly by using the weighted model. The coefficient is larger for the weighted model. This suggests that while previously, these stocks have not been that related, recently the stocks have become less balanced. This makes sense from what we saw when we plotted the two timeseries. This would be an increasing risk for our portfolio.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A python equivalent for R markdown</title>
      <link>https://jackbakerds.com/posts/python-equivalent-rmarkdown/</link>
      <pubDate>Tue, 04 May 2021 20:20:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/python-equivalent-rmarkdown/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R markdown&lt;/a&gt; is a powerful tool for sharing insights with stakeholders. You can write snippets of R code that generate plots. This can then be compiled to a HTML or pdf file that you can share with non-technical stakeholders.&lt;/p&gt;
&lt;p&gt;This is not as straightforward in python. Yes, Jupyter notebooks are a great way of sharing analysis with other developers. But compiling to HTML/pdf, with code snippets removed, that looks nice enough for a non-technical stakeholder, I&amp;rsquo;ve found clunky using Jupyter notebooks. R markdown also has great tools to generate &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/kable.html&#34;&gt;nice looking tables&lt;/a&gt;, not just plots.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve been working with a python heavy team though, so have been trying to figure out how to generate R markdown style documents. In this post I&amp;rsquo;ll outline what I&amp;rsquo;ve been using to generate HTML reports in python, that look nice enough to share with non-technical stakeholders. The process uses a few tools.&lt;/p&gt;
&lt;h2 id=&#34;step-1-embedding-plots-in-html&#34;&gt;
  Step 1: Embedding Plots in HTML
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-1-embedding-plots-in-html&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The first step is to embed plots into a static HTML that you can then share with others. A great tool for this is &lt;a href=&#34;https://plotly.com/python/&#34;&gt;plotly&lt;/a&gt;. Plotly has a &lt;a href=&#34;https://plotly.com/python-api-reference/generated/plotly.io.to_html.html&#34;&gt;&lt;code&gt;to_html&lt;/code&gt; function&lt;/a&gt; (one of my amazing colleagues found this) which will write the plots as a HTML string, which you can then write to a file.&lt;/p&gt;
&lt;p&gt;Plotly graphs look the part, and they allow the user to hover over the points to see what the values are; I&amp;rsquo;ve found this goes down well with customers.&lt;/p&gt;
&lt;p&gt;Sometimes users need plots they can copy-paste though. In this case I recommend using python&amp;rsquo;s more standard plotting libraries, like &lt;a href=&#34;https://seaborn.pydata.org/&#34;&gt;seaborn&lt;/a&gt; or &lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;. To embed the image into HTML without needing to have a separate image file the HTML references is a bit more involved. But you can do it by encoding the image as base64 and writing directly to your HTML. Just follow the instructions in this &lt;a href=&#34;https://stackoverflow.com/questions/48717794/matplotlib-embed-figures-in-auto-generated-html&#34;&gt;stackoverflow post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-2-setting-the-layout-of-the-html-document&#34;&gt;
  Step 2: Setting the Layout of the HTML Document
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-2-setting-the-layout-of-the-html-document&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Great, now you can embed plots in HTML, and actually if you turn off the &lt;code&gt;full_html&lt;/code&gt; option in plotly&amp;rsquo;s &lt;code&gt;to_html&lt;/code&gt; function, you can write as many plotly plots as you like to a html file by just appending the strings.&lt;/p&gt;
&lt;p&gt;But what about layout, commentary, and tables of data? This is where I use python&amp;rsquo;s HTML templating libraries &amp;ndash; which allow you to use python to generate static HTML files from templates. This technique is powerful, and used in python backend development libraries like &lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/&#34;&gt;flask&lt;/a&gt; and &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;django&lt;/a&gt;. It&amp;rsquo;s quick to learn, but does require some knowledge of HTML.&lt;/p&gt;
&lt;p&gt;I use the &lt;a href=&#34;https://jinja.palletsprojects.com/en/2.11.x/&#34;&gt;Jinja&lt;/a&gt; library to generate my HTML reports (it&amp;rsquo;s one of the most popular HTML templating libraries in python).&lt;/p&gt;
&lt;h2 id=&#34;step-3-making-the-report-look-nice&#34;&gt;
  Step 3: Making the Report Look Nice
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-3-making-the-report-look-nice&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;For anyone that&amp;rsquo;s worked with HTML before you&amp;rsquo;ll know it takes a long time to make anything which looks presentable.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t want to spend much time on this because I wanted to generate the report as quick as possible. So I used the &lt;a href=&#34;https://getbootstrap.com/&#34;&gt;bootstrap&lt;/a&gt; CSS library. This tool allows you to make your report look nice in a short amount of time (all I tend to do is wrap everything in a &lt;code&gt;&amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;&lt;/code&gt; and maybe add some padding).&lt;/p&gt;
&lt;p&gt;Bootstrap is basically a set of prebuilt HTML classes you can use to format your HTML. For example to make an HTML table look nice in bootstrap it&amp;rsquo;s as simple as &lt;code&gt;&amp;lt;table class=&amp;quot;table&amp;quot;&amp;gt;&lt;/code&gt; (tables look quite ugly in standard HTML). You can add padding at the top of a title element by adding &lt;code&gt;&amp;lt;h1 class=&amp;quot;pt-1&amp;quot;&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;thoughts&#34;&gt;
  Thoughts
  &lt;a class=&#34;heading-link&#34; href=&#34;#thoughts&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;As you can see, this is a fiddlier process than with R markdown (if anyone has a better way, please get in touch!). But the tools are useful to learn, especially Jinja and bootstrap which are standard tools for web development. Once you&amp;rsquo;ve learnt the libraries, and have some pre-made templates ready to go, the process gets quite quick.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three ways to speed up developing your data science solutions</title>
      <link>https://jackbakerds.com/posts/speed-up-data-science-development/</link>
      <pubDate>Mon, 26 Apr 2021 21:00:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/speed-up-data-science-development/</guid>
      <description>&lt;p&gt;Your customers want results and quickly. This can be stressful as a data scientist: solutions are often experimental and results not guaranteed; you need time to think about the problem.&lt;/p&gt;
&lt;p&gt;In this post I&amp;rsquo;ll share three ways I use to develop data science solutions faster.&lt;/p&gt;
&lt;p&gt;I find there are a few time sinks when developing data science solutions: going down a rabbit hole or dead end, productionising, checking your solution actually beats the current business process, and ensuring your code is free of bugs. How do I try to avoid these pitfalls?&lt;/p&gt;
&lt;h2 id=&#34;1-start-as-simple-as-possible&#34;&gt;
  1. Start as simple as possible
  &lt;a class=&#34;heading-link&#34; href=&#34;#1-start-as-simple-as-possible&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;When I start a new problem, I implement the simplest possible solution I can think of. Once this is developed and tested, I iteratively improve on it. What this gets you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A baseline you can compare all your models against. This helps stop you going down a rabbit hole: I talk about this more later.&lt;/li&gt;
&lt;li&gt;A quick, quality start &amp;ndash; I regularly find a baseline beats a more complex model.&lt;/li&gt;
&lt;li&gt;A chance to develop the pipeline without worrying about intricate model headaches. I find this makes a solution easier to productionise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This also applies to iterations on your baseline. Break these down so they&amp;rsquo;re as small as possible. If you follow the third point this becomes powerful.&lt;/p&gt;
&lt;h2 id=&#34;2-modularise&#34;&gt;
  2. Modularise
  &lt;a class=&#34;heading-link&#34; href=&#34;#2-modularise&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Often a data science solution consists of lots of mini problems you need to solve. I recommend you break down your problem as much as possible into components. What this gets you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can write each modular part as separate code modules. Set these to take relatively general input and output, and your code will be much easier to productionise: all you need to write are interfaces to your particular infrastructure (which is reusable code).&lt;/li&gt;
&lt;li&gt;Your code will be easier to test, which helps ensure it&amp;rsquo;s bug free. For example using the technique I talk about &lt;a href=&#34;https://jackcbaker.github.io/posts/check-data-science-pipeline-working/&#34;&gt;in this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you follow the first point, your code will be reusable for future solutions.&lt;/li&gt;
&lt;li&gt;Your baseline becomes even simpler :).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, say you&amp;rsquo;re developing a churn model, you might have: one or two modules handling data transformation, a module that handles the model build, one that handles serving predictions, and one that handles customer visualisations.&lt;/p&gt;
&lt;h2 id=&#34;3-test-each-iteration-against-your-baseline-the-current-business-process-and-your-best-model-so-far&#34;&gt;
  3. Test each iteration against your baseline, the current business process and your best model so far
  &lt;a class=&#34;heading-link&#34; href=&#34;#3-test-each-iteration-against-your-baseline-the-current-business-process-and-your-best-model-so-far&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;If you follow what I mentioned earlier and keep your model iterations small, this can really keep you on track:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You catch rabbit holes early by seeing that an iteration made no improvement.&lt;/li&gt;
&lt;li&gt;You can hone in on which module needs most improvement.&lt;/li&gt;
&lt;li&gt;You can fail fast &amp;ndash; if something isn&amp;rsquo;t working, you&amp;rsquo;ll know it quickly and can manage customer expectations.&lt;/li&gt;
&lt;li&gt;You have regular tangible results to share with customers.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to move your data science solution into a hands off state</title>
      <link>https://jackbakerds.com/posts/data-science-solution-to-hands-off-state/</link>
      <pubDate>Sat, 17 Apr 2021 15:36:34 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/data-science-solution-to-hands-off-state/</guid>
      <description>
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/hands-off/relaxed.jpg&#34; alt=&#34;A person relaxing next to a lake&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Photo by 
        &lt;a href=&#34;https://unsplash.com/@simonmigaj?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt; 
            S Migaj on Unsplash.
        &lt;/a&gt; 
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;Once you&amp;rsquo;ve developed a data science solution it&amp;rsquo;s easy to tinker with it indefinitely: running it manually, making small changes to the output, iterating on the model. This is not ideal - once you have a solution you&amp;rsquo;re happy with ideally you should be able to leave it running with minimal input from you, unless something falls over.&lt;/p&gt;
&lt;p&gt;Whether you&amp;rsquo;re handing over to a support team, or will be supporting the tool yourself, this post gives some tips when you&amp;rsquo;re trying to move your solution into a hands-off state - or putting it into production.&lt;/p&gt;
&lt;p&gt;Firstly you want to be logging. Logging is where output from your code is printed to a file so you (or the support team) can refer to it later. To move your code into a hands-off state you want to have logs that save to a file, so you can inspect these later if something goes wrong with the tool.&lt;/p&gt;
&lt;p&gt;What should you be logging? Any obvious things that might go wrong with the tool, such as data validation input, or the tool not being able to connect to a web service, etc. should be flagged as a warning or error and logged. Another good rule of thumb is to flag any &amp;lsquo;transaction&amp;rsquo;. What I mean by this is things like reading files, connecting to a database or calling an API. These are all things that can fail and help serve as a rough guide for where your code failed and what likely went wrong. It&amp;rsquo;s a good idea to have a summary at the end of a log indicating if the tool finished successfully, and if it failed, what went wrong. A log summary is really appreciated by support teams.&lt;/p&gt;
&lt;p&gt;Another useful tool is alerting. This is where your tool notifies you (or the support team) when something went wrong or failed. One of the most effective ways of doing this is to get your tool to message you on slack or MS teams when something&amp;rsquo;s gone wrong. This is very easy to do using webhooks. You can also use email for this e.g. using AWS SNS. It&amp;rsquo;s a good idea to notify you when something&amp;rsquo;s gone wrong, as well as when the tool is started and finished; then absence of these alerts will also tell you something is wrong.&lt;/p&gt;
&lt;p&gt;Triage tables are invaluable, especially if you are handing over to a support team or someone is keeping an eye on the tool while you&amp;rsquo;re away. Triage tables list the most common things that can go wrong with the tool, as well as the team that should be contacted to resolve this issue. For example if your tool relies on a data feed that&amp;rsquo;s managed by another team; if this breaks you can request support to contact that team directly. That way the issue can be resolved without you acting as an intermediary which wastes time for everyone. The act of doing this also forces you to think carefully about your tool and what error handling should be in place, to make sure it&amp;rsquo;s clear from the logs what went wrong.&lt;/p&gt;
&lt;p&gt;Finally, you have to make sure you have a business&amp;rsquo;s best interests at heart. It&amp;rsquo;s easy as a data scientist to make constant iterations to a model, or to agree to every request a customer asks for. But at the end of the day, your job is to provide as much value to a business as possible. You want a model to be effective, but you don&amp;rsquo;t need it to be perfect, after a point you can probably provide more value working on a different solution. Likewise, it&amp;rsquo;s imperative to listen to what the customer wants, but after a time of ensuring you have met customer requirements, you might be able to provide better value by building the customer a different tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to tell your data science pipeline is actually working?</title>
      <link>https://jackbakerds.com/posts/check-data-science-pipeline-working/</link>
      <pubDate>Sun, 11 Apr 2021 15:36:34 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/check-data-science-pipeline-working/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been there - studying model output for hours to check a model is working. There&amp;rsquo;s some better ways than this. For example designing backtests to check a model does better than a baseline. The trouble with these methods is they don&amp;rsquo;t categorically show that a model doing exactly as it is designed to do; they&amp;rsquo;re also not automated, or (as with a backtest) are expensive to run; and sometimes they&amp;rsquo;re harder to design than the model itself! Tests that are not automated mean as soon as you make changes you need to do another lengthy check or risk the model being invalid. Other options are unit and integration tests, which are automated, but these don&amp;rsquo;t explicitly check the model is working correctly.&lt;/p&gt;
&lt;p&gt;The way I solve this problem is test datasets. We often spend a lot of time as data scientists waiting for data to arrive. Why not create a test dataset during that time (it only takes about an hour for simple pipelines, which is a lot less than typical data lead time). This test dataset should: be modelled off the schema of your source data; have output that, once processed through your modelling pipeline, is known exactly. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you have a regression pipeline, you could generate random linearly related data and check held-out predictions match your actuals within a desired level of tolerance.&lt;/li&gt;
&lt;li&gt;If you have a forecasting pipeline, you could check it can predict on timeseries data generated from an ARIMA process within a desired level of tolerance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This does not replace the function of a backtest: it is not checking if the model actually works well on the problem at hand. But what it does is check that your modelling pipeline is doing exactly what it&amp;rsquo;s designed to do. This is especially important when you have quite a complex pipeline which is doing multiple data transformations before applying the model. It&amp;rsquo;s easy to make a mistake with these transformations and not notice. The test can also be automated, and is cheap to run (if you keep your test dataset small), so you get a free integration test on your full pipeline. That way if you make any changes you can just run the test again to check things are still working as expected. You can also add some tests in between different stages of the pipeline to get integration tests on different parts of the pipeline for free.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to include libraries in your project while also editing them</title>
      <link>https://jackbakerds.com/posts/include-libraries-project-editing/</link>
      <pubDate>Tue, 06 Apr 2021 18:29:47 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/include-libraries-project-editing/</guid>
      <description>&lt;p&gt;I came across this issue the other day: I wanted to include a library in a project, but to make it work in my code I would have to edit it. I then needed to write those changes back to the original library, with minimal faff. This post explains the best way I found of doing this.&lt;/p&gt;
&lt;p&gt;An obvious solution is to include the whole codebase of the library into your project. This will allow you to make the required changes. This can be quite messy though: your repo can become very large if you&amp;rsquo;re working with a big library, and it&amp;rsquo;s a pain to push your changes back to the original project.&lt;/p&gt;
&lt;p&gt;I found a better way of doing this: &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;&lt;code&gt;git submodule&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;git submodule&lt;/code&gt; allows you to manage additional repos within your project. Using it, the submodule repo is contained in a directory in your own project. By going into the directory that contains that repo you can make changes to that module using the standard git workflow, and push them back to the original library.&lt;/p&gt;
&lt;p&gt;When you push your own project repo, it will just contain a reference to the libraries repo location, and a commit-id. This keeps it small and concise. A great tutorial on how &lt;code&gt;git submodule&lt;/code&gt; works is &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a big push for reusability in code at the moment. This is a great way to keep your code as small reusable components rather than a big monolith.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://jackbakerds.com/about/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jackbakerds.com/about/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m a data scientist based in Manchester. This site is a blog about data science and developing. The views expressed are my own.&lt;/p&gt;
&lt;p&gt;Before becoming a data scientist, I was a PhD student in statistical machine learning at &lt;a href=&#34;https://www.lancaster.ac.uk/stor-i/&#34;&gt;STOR-i&lt;/a&gt; Centre for Doctoral Training at Lancaster University. I studied Markov Chain Monte Carlo (MCMC) approaches for large datasets. MCMC is the most popular approach for training Bayesian models, but it does not scale well to large datasets.&lt;/p&gt;
&lt;p&gt;Email: jackcbaker &amp;lt;at&amp;gt; yahoo &amp;lt;dot&amp;gt; com&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>