<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jack Baker</title>
    <link>https://jackbakerds.com/posts/</link>
    <description>Recent content in Posts on Jack Baker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 18 Sep 2021 18:18:31 +0100</lastBuildDate><atom:link href="https://jackbakerds.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A heuristic for prioritising demand planning at lower granularities</title>
      <link>https://jackbakerds.com/posts/inventory-prioritisation/</link>
      <pubDate>Sat, 18 Sep 2021 18:18:31 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/inventory-prioritisation/</guid>
      <description>&lt;p&gt;Most inventory optimisation methods tell you how much stock to bring in at the network level. But often at lower granularities there is not enough space, or enough trucks to move it all. In this case you need to prioritise what to bring in. This article develops a heuristic to do this.&lt;/p&gt;
&lt;h1 id=&#34;why-supply-chain-networks-are-often-strained&#34;&gt;
  Why supply chain networks are often strained?
  &lt;a class=&#34;heading-link&#34; href=&#34;#why-supply-chain-networks-are-often-strained&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;Most demand planning is performed at the highest level of granularity (e.g. how much will be demanded in the UK over the next week). This reduces the volatility of demand and so means less unnecessary stock is produced or bought. But this is only part of the supply chain. A big part is then distributing the stock around the supply network to ensure it is as close to the customer as possible.&lt;/p&gt;
&lt;p&gt;Since there is only enough stock at the highest granularity, this causes a strain on the network because:&lt;/p&gt;
&lt;h3 id=&#34;1-the-demand-at-lower-granularities-will-be-more-volatile&#34;&gt;
  1. The demand at lower granularities will be more volatile
  &lt;a class=&#34;heading-link&#34; href=&#34;#1-the-demand-at-lower-granularities-will-be-more-volatile&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;To fill each warehouse with enough stock to account for these volatilities would be more stock than is available in the network, since demand planning performed at the highest level of granularity.&lt;/p&gt;
&lt;h3 id=&#34;2-warehouse-capacities&#34;&gt;
  2. Warehouse capacities
  &lt;a class=&#34;heading-link&#34; href=&#34;#2-warehouse-capacities&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Often individual warehouse capacities are tight: for example they might be full of a product that hasn&amp;rsquo;t had as much demand as expected etc.&lt;/p&gt;
&lt;h3 id=&#34;3-capacity-for-moving-stock&#34;&gt;
  3. Capacity for moving stock
  &lt;a class=&#34;heading-link&#34; href=&#34;#3-capacity-for-moving-stock&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;The capacity to move stock around the network is limited (e.g. the number of trucks available). Because the network is strained stock might be put in the wrong warehouse and then has to be moved. But this is a waste that is bad for profitability and the environment, so should be minimised if possible.&lt;/p&gt;
&lt;h3 id=&#34;4-seasonality&#34;&gt;
  4. Seasonality
  &lt;a class=&#34;heading-link&#34; href=&#34;#4-seasonality&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Warehouses will tend to have highly busy days followed by quieter days. Often the capacity for moving stock will be enough for a day of &amp;lsquo;average demand&amp;rsquo;. This adds a big problem: when should I start bringing in stock for my busiest day? Because if you don&amp;rsquo;t, you&amp;rsquo;ll miss a lot of sales as you didn&amp;rsquo;t bring enough stock in on time.&lt;/p&gt;
&lt;h1 id=&#34;why-traditional-inventory-optimisation-methods-dont-work-at-the-network-level&#34;&gt;
  Why traditional inventory optimisation methods don&amp;rsquo;t work at the network level?
  &lt;a class=&#34;heading-link&#34; href=&#34;#why-traditional-inventory-optimisation-methods-dont-work-at-the-network-level&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;The classic inventory optimisation methods such as the &lt;em&gt;reorder point&lt;/em&gt; method, and the &lt;em&gt;(s, S)&lt;/em&gt; method answer the question:&lt;/p&gt;
&lt;h4 id=&#34;given-my-uncertain-demand-how-much-inventory-should-i-bring-into-my-warehouse-to-make-sure-im-unlikely-to-miss-a-sale&#34;&gt;
  &amp;lsquo;Given my (uncertain) demand how much inventory should I bring into my warehouse to make sure I&amp;rsquo;m unlikely to miss a sale?&amp;rsquo;
  &lt;a class=&#34;heading-link&#34; href=&#34;#given-my-uncertain-demand-how-much-inventory-should-i-bring-into-my-warehouse-to-make-sure-im-unlikely-to-miss-a-sale&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h4&gt;
&lt;p&gt;But at the network level, this is probably the wrong question to ask. Because your network is strained, you&amp;rsquo;re unlikely to be able to bring in the stock these methods recommend at the warehouse level. The methods also focus on one item at a time, but at the network level, bringing in one item may mean you can&amp;rsquo;t bring in enough of another. This means you need to consider all items at the same time.&lt;/p&gt;
&lt;p&gt;In reality, the question often morphs into, what is the most valuable stock to bring into the warehouse today? This is the question I&amp;rsquo;ll be solving in this article.&lt;/p&gt;
&lt;h1 id=&#34;what-is-the-most-valuable-stock-to-bring-into-the-warehouse-today&#34;&gt;
  What is the most valuable stock to bring into the warehouse today?
  &lt;a class=&#34;heading-link&#34; href=&#34;#what-is-the-most-valuable-stock-to-bring-into-the-warehouse-today&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;In this article I&amp;rsquo;ll focus on a way to solve this problem. For now I&amp;rsquo;ll focus on what to bring into one warehouse with multiple items, random demand, and a constraint on how much can be bought in (e.g. the number of trucks available to move the product).&lt;/p&gt;
&lt;p&gt;This problem is not too bad to solve on its own, but typically supply networks will see seasonal demand (see point 4 above). This means we don&amp;rsquo;t just need to think about today, we need to look ahead to see if stock desperately needs to be brought in for the busiest day. This adds a lot of complexity, and the solution took a lot of thought; but I cover a heuristic for this case after the single day case.&lt;/p&gt;
&lt;p&gt;The solution can easily be extended to multiple warehouses competing for stock, using the same logic. I&amp;rsquo;ll leave this to a future post.&lt;/p&gt;
&lt;p&gt;It should also be extendable to the case where the warehouse capacity itself is the constraint.&lt;/p&gt;
&lt;h1 id=&#34;single-period-problem&#34;&gt;
  Single period problem
  &lt;a class=&#34;heading-link&#34; href=&#34;#single-period-problem&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;We&amp;rsquo;ll start with the single period problem (e.g. solving the problem for just one day), where we can derive an optimal solution in terms of expected profit. This sets up the notation for the multi-period problem.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;
  Setup
  &lt;a class=&#34;heading-link&#34; href=&#34;#setup&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Assume we have \(i= 1,&amp;hellip;,I\) SKUs. At the end of the period a random number \(Y_i\) of each item is demanded. Let&amp;rsquo;s assume we have a reasonable handle on \(\mathbb P(Y_i \geq y)\), i.e. the demand probabilities. This can be achieved through a probabilistic forecast.&lt;/p&gt;
&lt;p&gt;We assume starting inventory \(x_i\) for each item is known. The amount of inventory of item \(i\) we choose to bring in we label \(a_i\).&lt;/p&gt;
&lt;p&gt;With this in place, we can label the remaining variables, and define the problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(r_i\) &amp;ndash; the revenue gained if one unit of item \(i\) is demanded.&lt;/li&gt;
&lt;li&gt;\(C_i\) &amp;ndash; the cost of bringing one further unit of item \(i\) into the warehouse. This could be due to e.g. transport or purchase costs.&lt;/li&gt;
&lt;li&gt;\(K\) &amp;ndash; maximum number of items that can be brought into the warehouse: i.e., we required \(\sum_{i=1}^I a_i \leq K\). This could be due to movement capacity constraints (e.g. number of trucks). There might be an explicit warehouse capacity constraint, which could be easily added to the single period problem, but requires more work for the multiple period problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can define the problem as bringing in \(a_i\) in order to maximize profit, defined as
\[
\sum_{i=1}^I \left(r_i Y_i - a_i C_i \right),
\]
while staying below the movement constraint \(\sum_{i=1}^I a_i \leq K\).&lt;/p&gt;
&lt;p&gt;Bring in too much stock, and we are paying unnecessary cost \(C_i\). But not enough stock will lead to unmet demand, and lost sales. Sometimes a penalty for missed sales gets added to this problem, to indicate impact to customer loyalty. This can easily be done in this formulation.&lt;/p&gt;
&lt;p&gt;We can solve the single period problem to maximise expected reward using a simple iterative algorithm. First define \(\hat a_i\) to be the amount of item \(i\) scheduled to be moved so far by the algorithm, and initialise by setting equal to 0, \(a_i = 0\).&lt;/p&gt;
&lt;p&gt;We can calculate the marginal reward of loading an extra unit of item \(i\) exactly as
\[
r_i \mathbb P(Y_i &amp;gt; x_i + \hat a_i) - C_i.
\]
The first term is the revenue multiplied by the probability that additional unit would be ordered. The second term is the cost of loading that item.&lt;/p&gt;
&lt;p&gt;This marginal reward immediately gives rise to an iterative algorithm:&lt;/p&gt;
&lt;h3 id=&#34;single-period-algorithm&#34;&gt;
  Single period algorithm
  &lt;a class=&#34;heading-link&#34; href=&#34;#single-period-algorithm&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Initialise \(a_i = 0\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Until STOP do&lt;/p&gt;
&lt;p&gt;i.
\[
i_{best} = \text{argmax}_i \left[ r_i \mathbb P(Y_i &amp;gt; x_i + \hat a_i) - C_i \right]
\]
ii.
If for \(i_{best}\) \(r_i P(Y_i &amp;gt; x_i + \hat a_i) - C_i &amp;lt; 0\), or \(\sum_{i=1}^I \hat a_i \geq K\): STOP&lt;/p&gt;
&lt;p&gt;iii.
Load item \(i_{best}\): set \(\hat a_{i_{best}} = \hat a_{i_{best}} + 1\)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;multi-period-problem&#34;&gt;
  Multi-period problem
  &lt;a class=&#34;heading-link&#34; href=&#34;#multi-period-problem&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;This problem is more complex but if we want to handle seasonality it&amp;rsquo;s essential. Now we add in time \(t = 1, &amp;hellip;, T\). We set \(T\), the horizon, to be the seasonal period (e.g. 7 if we&amp;rsquo;re looking at daily data). This ensures the busiest days are taken into account when loading.&lt;/p&gt;
&lt;p&gt;We assume at the start of each period \(t\), we have inventory for item \(i\) of \(x_{ti}\). We assume \(x_{1i}\) is known, but for all later periods it depends on the demand and loading choice. So is unknown.&lt;/p&gt;
&lt;p&gt;We assume a loading amount \(a_{ti}\) is chosen for the period, and after that a random amount \(Y_{ti}\) is demanded. This is fulfilled from the inventory \(x_{ti} + a_{ti}\), with reward \(r_i\) for each unit of item \(i\) successfully fulfilled, but any demand over the inventory is lost sales.&lt;/p&gt;
&lt;p&gt;The rest of the problem setup is as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(C_i\) &amp;ndash; cost of loading one unit of item \(i\)&lt;/li&gt;
&lt;li&gt;\(K\) &amp;ndash; capacity of items that brought into the warehouse in any period, i.e. for all \(t\), \(\sum_{i=1}^I a_{ti} \leq K\)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;possible-heuristic-procedure&#34;&gt;
  Possible heuristic procedure
  &lt;a class=&#34;heading-link&#34; href=&#34;#possible-heuristic-procedure&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Because starting inventory \(x_{1i}\) is known, we can solve the problem exactly using dynamic programming. We would do this by proceeding backwards from the last horizon, which we solve using a single period problem. Then each of the possible cases for \(T-1\) (depending on inventory levels at that point) can be solved using a single period problem, and the value function for the expected remaining inventory at \(T\), etc.&lt;/p&gt;
&lt;p&gt;The big problem with this approach is we have to consider all possible inventory levels for each item, which will quickly explode as the number of items increases.&lt;/p&gt;
&lt;p&gt;We attempt to get around this by noticing that we are only interested in \(a_{1i}\) at each period, i.e. the loading we have to do today. To do this we have to consider the future states in case there&amp;rsquo;s a particularly busy day. But we suggest instead of having it as a state, we replace it with its expected value, determined by \(x_{ti}\), \(\hat a_{ti}\) and \(\mathbb E(Y_{ti})\). Once again we defined \(\hat a_{ti}\) to be the loadings chosen by the algorithm so far.&lt;/p&gt;
&lt;p&gt;We can initialise the problem using a greedy algorithm. Set \(\hat a_{1i}\) by solving the single period problem for period 1. Now initialise the inventory for period 2 using expected values
\[
x_{2i} = \max\left[x_{1i} + \hat a_{1i} - \mathbb E(Y_{1i}), 0 \right].
\]&lt;/p&gt;
&lt;p&gt;We can proceed initialisation iteratively using a similar procedure: solving a single period problem for horizon \(t\) to get an initial \(\hat a_{ti}\), then initialising the inventory as
\[
x_{t+1,i} = \max\left[x_{1i} + \sum_{s=1}^t \left(\hat a_{si} - \mathbb E(Y_{si})\right), 0 \right].
\]&lt;/p&gt;
&lt;p&gt;Now we&amp;rsquo;ve intialised the problem, we can proceed to the heuristic algorithm. For this, the inventory \(x_{ti}\) and loading amounts \(\hat a_{ti}\), need to be constantly updated as things are changed by the algorithm.&lt;/p&gt;
&lt;p&gt;Similar to dynamic programming, we proceed backwards, but now with our fixed inventory states. For period \(t=T\), we are at the end of the horizon, so have no horizons ahead to consider. So it&amp;rsquo;s safe to assume that the single period problem will be a good result. Therefore we do nothing for this period, and move back to the next period.&lt;/p&gt;
&lt;p&gt;At period \(t=T-1\), we need to look at both the current period, and the period \(t=T\). First set \(b_{t,i} = x_{ti} + \hat a_{ti}\). Applying our inventory assumption, similar to the single period problem, we can write down the marginal reward of adding another unit of item \(i\) to the loading in period \(T-1\):
\[
N_{T-1,i} = \mathbb P(Y_{T-1,i} &amp;gt; b_{T-1, i}) R_i - C_i +
\mathbb P(Y_{T-1,i} \leq b_{T-1, i}) \mathbb P(Y_{T,i} &amp;gt; b_{T,i}) R_i.
\]
We can break down the parts of the marginal reward as follows: the additional unit of product $i$ will either be ordered at time $T-1$, not ordered at time $T-1$ but ordered at time $T$; or not ordered at all because we&amp;rsquo;ve reached the end of the horizon.&lt;/p&gt;
&lt;p&gt;Because we&amp;rsquo;ve initialised this problem with the single period algorithm though, our loadings might be full. In this case we can&amp;rsquo;t just add the item with the best marginal reward, we need to swap it with an item in the loading.&lt;/p&gt;
&lt;p&gt;To do this we can check the marginal cost of removing one unit of one item from the knapsack. This is equivalant to the marginal reward  above replacing \(b_{T-1, i}\) with \(b_{T-1, i} - 1\); i.e.
\[
M_{T-1,i} = \mathbb P(Y_{T-1,i} &amp;gt; b_{T-1, i} - 1) R_i - C_i +
\mathbb P(Y_{T-1,i} \leq b_{T-1, i} - 1) \mathbb P(Y_{T,i} &amp;gt; b_{T,i} - 1) R_i.
\]&lt;/p&gt;
&lt;p&gt;A single step of the algorithm then proceeds as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Check if \(N^* = \max_{i} N_{T-1,i}\) is positive. If it&amp;rsquo;s \(\leq 0\), STOP.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set \(i_{best} = \text{argmax}_{i} N_{T-1,i}\). Set \(M_* = \min M_{T-1, i}\), and \(j_{worst} = \text{argmin}_{i} M_{T-1,i}\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the capacity \(K\) is not reached (i.e. \(\sum_{i=1}^I a_{T-1,i} &amp;lt; K\)), then add \(i_{best}\) to the loading. Set \(a_{T-1,i_{best}} = a_{T-1, i_{best}} + 1\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the capacity \(K\) is reached, then check if \(N^* &amp;gt; M_*\). If it isn&amp;rsquo;t STOP. Otherwise load \(i_{best}\) and unload \(j_{worst}\); i.e. set \(\hat a_{T-1, i_{best}} = \hat a_{T-1, i_{best}} + 1\) and \(\hat a_{T-1, j_{worst}} = \hat a_{T-1, j_{worst}} - 1\)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After each iteration of this algorithm, if \(N^* \leq 0\), or \(N^* \leq M_*\), we stop completely and move to the next period.&lt;/p&gt;
&lt;p&gt;Otherwise we update the inventory values \(x_{t,i_{best}}\) and \(x_{t, j_{worst}}\) for \(t = T-1, T\) using the new loadings \(\hat a_{T-1, i_{best}}\) and \(\hat a_{T-1, j_{worst}}\). Also update \(N_{T-1, i_{best}}\), \(N_{T-1, j_{worst}}\), \(M_{T-1, i_{best}}\) and \(M_{T-1, i_{worst}}\) given the updated loadings and inventory values. Then we repeat the algorithm above, until either \(N^* \leq 0\), or \(N^* \leq M_*\).&lt;/p&gt;
&lt;p&gt;Moving to time period $t = T-2$ we can follow the exact logic as for $t = T-1$. The difference here will be our marginal rewards. We have
\[
N_{T-2,i} = \mathbb P(Y_{T-2,i} &amp;gt; b_{T-2, i}) R_i - C_i
+ \mathbb P(Y_{T-2,i} \leq b_{T-2, i}) \mathbb P(Y_{T-1,i} &amp;gt; b_{T-1,i}) R_i
+ \mathbb P(Y_{T-2,i} \leq b_{T-2, i}) \mathbb P(Y_{T-1,i} \leq b_{T-1, i}) \mathbb P(Y_{T,i} &amp;gt; b_{T,i}) R_i.
\]
Breaking this down: the additional unit of product \(i\) will either be ordered at time \(T-2\), not ordered at time \(T-2\) but ordered at time \(T-1\), not ordered at time \(T-2, T-1\) but ordered at time \(T\); or not ordered at all because we&amp;rsquo;ve reached the end of the horizon. Similar logic can be used to calculate \(M_{T-2, i}\).&lt;/p&gt;
&lt;p&gt;This procedure can be repeated to time period $t=1$, which will be our required loading for today.&lt;/p&gt;
&lt;h2 id=&#34;intuition&#34;&gt;
  Intuition
  &lt;a class=&#34;heading-link&#34; href=&#34;#intuition&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The intuition for this heuristic is that the uncertainty in demand has two effects: the uncertainty for that period; the uncertainty of how much inventory will be available in later periods. This heuristic captures how demand affects the uncertainty in each period, but ignores knock-on uncertainty effects on the inventory levels &amp;ndash; replacing it with a deterministic expected value. This should work because we are mainly interested in the first period, so not capturing the inventory uncertainty would hopefully not have a massive effect.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;
  Acknowledgments
  &lt;a class=&#34;heading-link&#34; href=&#34;#acknowledgments&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Thanks to Jake Clarkson for reviewing this algorithm and providing helpful comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature selection for forecasting algorithms</title>
      <link>https://jackbakerds.com/posts/forecasting-feature-selection/</link>
      <pubDate>Mon, 23 Aug 2021 20:48:31 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/forecasting-feature-selection/</guid>
      <description>&lt;p&gt;In this article, we talk feature selection for forecasting. We start by explaining why you should
think twice before adding features to your forecasts; then we talk
about some ways to do feature selection for forecasting models. To put things in practical terms, we demo
a feature selection method that has a decent balance between performance and computational
cost in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;why-you-should-think-twice-before-including-features-in-your-forecasting-models&#34;&gt;
  Why you should think twice before including features in your forecasting models?
  &lt;a class=&#34;heading-link&#34; href=&#34;#why-you-should-think-twice-before-including-features-in-your-forecasting-models&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;For many data science models, the more features (often referred to as
regressors in forecasting), the better your model accuracy will be. But
forecasting is quite finickity, and this is often not the case.&lt;/p&gt;
&lt;h3 id=&#34;why-is-this&#34;&gt;
  Why is this?
  &lt;a class=&#34;heading-link&#34; href=&#34;#why-is-this&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;We’ll break this down into a few points.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let’s imagine you want to forecast whether a coin will land heads or
tails. That’s going to be a non-starter. This is because the
variable you’re trying to forecast (let’s call it the target), is
completely random. Some targets just don’t have any signal at all,
or don’t have any regressors that provide any signal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let’s call the observations of the variable you have already seen
the actuals. These will often contain a surprising amount of
information already. The forecasting algorithm you’re using will be
hell bent on picking up on this signal. Which means your feature
doesn’t just have to be correlated with the target, it has to
provide information that the actuals don’t. This is much rarer. For
example, ice cream sales will be correlated with the weather, but it
definitely won’t contain any more information than in the actuals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You either need to know your regressor ahead of time (it needs to be
&lt;em&gt;forward facing&lt;/em&gt;), or you need to forecast that regressor as well.
Going back to the ice cream example, this regressor is not forward
facing. So if you want to know the weather a week from now, you’ll
have to forecast ice cream sales ahead by a week, and then feed that
into your forecasting algorithm. This adds a lot of noise to your
regressor, which in turn adds noise to your forecast and makes it
worse. It is rarely worth doing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even forward facing regressors frequently add unintended noise to
your forecast, just because there is not enough signal, similar to
points 1 and 2.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;some-ways-to-perform-feature-selection-for-a-forecasting-algorithm&#34;&gt;
  Some ways to perform feature selection for a forecasting algorithm
  &lt;a class=&#34;heading-link&#34; href=&#34;#some-ways-to-perform-feature-selection-for-a-forecasting-algorithm&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;There are a few ways to check if a regressor provides any useful
information to a forecast. Most of these rely on a &lt;em&gt;walkforward
backtest&lt;/em&gt;. A walkforward backtest is where you start at a historic date
(called the train upto date), you forecast using only data up to that
date, then you move up to the next date and repeat, until you reach the
present.&lt;/p&gt;
&lt;p&gt;Then to evaluate your forecast, you can calculate the forecast error
compared with actuals. So it is a lot like a train/test set split in
classic data science problems. A tutorial of this technique is
&lt;a href=&#34;https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/&#34;&gt;here&lt;/a&gt;.
If you have two forecasts you want to decide between, you can run a
backtest for each of them. The one with the lowest error would be
classed as the best.&lt;/p&gt;
&lt;p&gt;With this, here are some ways to perform feature selection, in order of
increasing computational and time cost:&lt;/p&gt;
&lt;h3 id=&#34;1-look-at-correlations-between-the-actuals-and-the-regressor&#34;&gt;
  1. Look at correlations between the actuals and the regressor.
  &lt;a class=&#34;heading-link&#34; href=&#34;#1-look-at-correlations-between-the-actuals-and-the-regressor&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;This just checks the regressor using in-sample fit, and does not cover
point 2 in the previous section (does the regressor contain information
the forecast doesn’t). This option is not recommended.&lt;/p&gt;
&lt;h3 id=&#34;2-use-the-akaike-information-criterion-aic-of-the-forecast&#34;&gt;
  2. Use the Akaike Information Criterion (AIC) of the forecast.
  &lt;a class=&#34;heading-link&#34; href=&#34;#2-use-the-akaike-information-criterion-aic-of-the-forecast&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;This is a statistical approximation that tries to use insample fit to
approximate predictive fit. See &lt;a href=&#34;https://otexts.com/fpp3/estimation-and-model-selection.html&#34;&gt;this
chapter&lt;/a&gt;
for using the AIC for model selection.&lt;/p&gt;
&lt;p&gt;The way this would work is you would fit your forecasting model both
with and without the regressor, and check the AIC of each. The model
with the lowest AIC would be the one you would pick.&lt;/p&gt;
&lt;p&gt;This method is good, fast, and commonly used. But it is an approximation
that only uses insample fit, so may not give a good estimate of
predictive fit.&lt;/p&gt;
&lt;h3 id=&#34;3-use-linear-regression-on-the-actuals-with-both-the-current-forecast-and-your-regressor-as-a-feature&#34;&gt;
  3. Use linear regression on the actuals with both the current forecast and your regressor as a feature
  &lt;a class=&#34;heading-link&#34; href=&#34;#3-use-linear-regression-on-the-actuals-with-both-the-current-forecast-and-your-regressor-as-a-feature&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;The key to this method is getting around point 2. We want to check how
much additional predictive information our regressor gives after taking
into account the information the forecast has learned.&lt;/p&gt;
&lt;p&gt;For this method we run a single backtest using the current forecast
without regressors. Then we feed both the regressor and the forecast
through a linear regression that is trying to learn the actuals.&lt;/p&gt;
&lt;p&gt;To assess how good the regressor is, we can examine whether the
regressor is significant in the standard way you would assess a linear
regression (i.e. check its p-value is sufficiently small). Because we
have also included the forecast itself in the regression, this
regression should account for what is already learned by the forecasting
algorithm.&lt;/p&gt;
&lt;p&gt;This method assesses predictive fit directly, but only needs to run one
backtest. The method works particularly well when you have many
regressors to check. You still just have to fit one backtest (the
current forecast with no regressors), then you can use standard
techniques for model selection in linear regression, such as recursive
feature elimination (make sure you don’t eliminate your forecast
though)!&lt;/p&gt;
&lt;h3 id=&#34;4-run-a-forecast-backtest-with-and-without-the-regressor&#34;&gt;
  4. Run a forecast backtest with and without the regressor
  &lt;a class=&#34;heading-link&#34; href=&#34;#4-run-a-forecast-backtest-with-and-without-the-regressor&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;This method is quite simple, we run a backtest both with a forecast
using the regressor, and without. We then compare the error of the two.
If the error of the forecast with the regressor is significantly lower,
we might use that regressor going forward.&lt;/p&gt;
&lt;p&gt;Backtests are bread and butter of model selection in forecasting. So
this method works well.&lt;/p&gt;
&lt;p&gt;But as the number of regressors goes up, this method gets prohibitively
expensive. We have to fit multiple forecasting models on each of the
train upto dates.&lt;/p&gt;
&lt;h1 id=&#34;demoing-method-3&#34;&gt;
  Demoing method 3
  &lt;a class=&#34;heading-link&#34; href=&#34;#demoing-method-3&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;We choose to demo method 3, as it offers a good balance between directly
measuring predictive quality of a regressor, with computational cost.
Often a backtest of the best forecasting method so far is already at
hand from other experiments, which is the most computationally demanding
part of this method.&lt;/p&gt;
&lt;p&gt;To demo this practically, we’ll use &lt;code&gt;R&lt;/code&gt;. Follow along with the &lt;a href=&#34;https://github.com/jackcbaker/blog-notebooks/blob/main/checking_regressors.Rmd&#34;&gt;interactive Rmarkdown&lt;/a&gt; for this tutorial on &lt;a href=&#34;https://github.com/jackcbaker&#34;&gt;my github&lt;/a&gt;. For all the packages used in
this tutorial, download the &lt;a href=&#34;https://mran.microsoft.com/snapshot/2020-10-09/web/packages/fpp3/index.html&#34;&gt;FPP3
package&lt;/a&gt;
by running in &lt;code&gt;R&lt;/code&gt; &lt;code&gt;install.packages(&#39;fpp3&#39;)&lt;/code&gt;. This package is a
companion to the amazing &lt;a href=&#34;https://otexts.com/fpp3/&#34;&gt;introductory forecasting
book&lt;/a&gt; by Rob Hyndman.&lt;/p&gt;
&lt;p&gt;The data we’ll use, is the US consumption expenditure data from the
book. This is a time series of quarterly percentage changes of various
measures in the US economy. The one we’re interested in forecasting is
economic production, which is a measure of the health of the US economy.
It can be accessed after loading in &lt;code&gt;fpp3&lt;/code&gt;, and is called &lt;code&gt;us_change&lt;/code&gt;.
The column we want to forecast is &lt;code&gt;Production&lt;/code&gt;. Let’s plot it&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(fpp3)

us_change &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;autoplot&lt;/span&gt;(Production) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;labs&lt;/span&gt;(y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Production (% change)&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/feature-selection-forecasting/plot_actuals-1.png&#34; alt=&#34;Plot of US % change production over time&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Plot of US % change production over time.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;To forecast this data, we’ll fit an &lt;a href=&#34;https://otexts.com/fpp3/expsmooth.html&#34;&gt;ARIMA
model&lt;/a&gt; to this data, which
allows regressors to be added.&lt;/p&gt;
&lt;p&gt;To fit the linear regression on the features we need to check, we’ll
need to run a backtest on our forecasting algorithm with no regressors.
Let’s backtest our forecasting model on the last 50 data points, and
forecast one-step ahead. The &lt;code&gt;ARIMA&lt;/code&gt; function will automatically perform
parameter tuning for the ARIMA model using AIC.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set seed for reproducibility&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;set.seed&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;)
backtest_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nrow&lt;/span&gt;(us_change)
backtest_results &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; us_change&lt;span style=&#34;color:#a6e22e&#34;&gt;[&lt;/span&gt;(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (backtest_size &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;n,]
backtest_results&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;forecast &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;, backtest_size)
&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(backtest_num in backtest_size&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) {
    fit &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; us_change[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; backtest_num),] &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;select&lt;/span&gt;(Production) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;model&lt;/span&gt;(
            forecast &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ARIMA&lt;/span&gt;(Production)
        )
    backtest_fc &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; fit &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forecast&lt;/span&gt;(h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    results_index &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; backtest_size &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (backtest_num &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    backtest_results&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;forecast[results_index] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; backtest_fc&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;.mean
}

&lt;span style=&#34;color:#75715e&#34;&gt;# Let’s plot our backtest results against actuals&lt;/span&gt;

backtest_results &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;pivot_longer&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(Production, forecast), names_to &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Type&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;autoplot&lt;/span&gt;(value)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/feature-selection-forecasting/backtest_plot-1.png&#34; alt=&#34;Plot of ARIMA backtest without regressors&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Plot of ARIMA backtest without regressors.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;Notice that the forecast does fine during normal periods, but performs
poorly during periods of large fluctuation. Can we use regressors to
improve this?&lt;/p&gt;
&lt;p&gt;We have modelled production data, which is typically measured using GDP.
But measuring GDP takes an enormous effort, and in many countries is
only done every quarter. Other data can be easier to estimate more
regularly, and is more readily available. One measure this might be true
for is consumer spending. This is the &lt;code&gt;Consumption&lt;/code&gt; column in our
dataset.&lt;/p&gt;
&lt;p&gt;Let’s use a simple linear regression to see if using consumption as a
regressor has any promise.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;regress_test &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lm&lt;/span&gt;(Production &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; forecast &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; Consumption, data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; backtest_results)
&lt;span style=&#34;color:#a6e22e&#34;&gt;summary&lt;/span&gt;(regress_test)

&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Call:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## lm(formula = Production ~ forecast + Consumption, data = backtest_results)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Residuals:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##     Min      1Q  Median      3Q     Max &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## -2.4347 -0.7459  0.1566  0.5379  2.4470 &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Coefficients:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##             Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## (Intercept)  -0.8588     0.1889  -4.545 3.84e-05 ***&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## forecast      0.8750     0.1406   6.224 1.23e-07 ***&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Consumption   1.2630     0.3158   4.000 0.000223 ***&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## ---&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Residual standard error: 0.9268 on 47 degrees of freedom&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Multiple R-squared:  0.6461, Adjusted R-squared:  0.6311 &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## F-statistic: 42.91 on 2 and 47 DF,  p-value: 2.501e-11&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The p-value is very low for the &lt;code&gt;Consumption&lt;/code&gt; column, well below the
classic 0.05 level. This indicates including it in the forecast should
improve predictive performance.&lt;/p&gt;
&lt;p&gt;What about consumer savings (the &lt;code&gt;Savings&lt;/code&gt; column in the dataset)?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;regress_test_savings &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lm&lt;/span&gt;(Production &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; forecast &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; Savings, data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; backtest_results)
&lt;span style=&#34;color:#a6e22e&#34;&gt;summary&lt;/span&gt;(regress_test_savings)

&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Call:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## lm(formula = Production ~ forecast + Savings, data = backtest_results)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Residuals:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##     Min      1Q  Median      3Q     Max &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## -3.1353 -0.5435  0.1223  0.6092  2.7229 &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Coefficients:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##             Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## (Intercept) -0.34043    0.16657  -2.044   0.0466 *  &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## forecast     1.08637    0.14867   7.307  2.8e-09 ***&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Savings     -0.01165    0.01029  -1.132   0.2633    &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## ---&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Residual standard error: 1.059 on 47 degrees of freedom&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## Multiple R-squared:  0.5383, Adjusted R-squared:  0.5186 &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## F-statistic: 27.39 on 2 and 47 DF,  p-value: 1.299e-08&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The p-value in this case is high, so we should not include savings in
our model.&lt;/p&gt;
&lt;p&gt;Let’s check our choice by including consumption in the ARIMA model and
running a new backtest&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;backtest_regressor_results &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; backtest_results
backtest_regressor_results&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;forecast_consumption &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;, backtest_size)
&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(backtest_num in backtest_size&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) {
    fit &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; us_change[1&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;(n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; backtest_num),] &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;select&lt;/span&gt;(Quarter, Production, Consumption) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;model&lt;/span&gt;(
            forecast &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ARIMA&lt;/span&gt;(Production &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; Consumption)
        )
    new_regressor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;select&lt;/span&gt;(us_change[n &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (backtest_num &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),], Quarter, Consumption)
    backtest_fc &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; fit &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forecast&lt;/span&gt;(new_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new_regressor)
    results_index &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; backtest_size &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (backtest_num &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    backtest_regressor_results&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;forecast_consumption[results_index] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; backtest_fc&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;.mean
}

&lt;span style=&#34;color:#75715e&#34;&gt;# Plot both backtests&lt;/span&gt;
backtest_regressor_results &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;pivot_longer&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(Production, forecast, forecast_consumption), names_to &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Type&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;autoplot&lt;/span&gt;(value)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/feature-selection-forecasting/regress_backtest-1.png&#34; alt=&#34;Plot of ARIMA backtest with and without selected regressors&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Plot of ARIMA backtest with and without selected regressors.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;It’s a little difficult to tell which forecast performed better by eye,
so let’s check the MSE error&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;backtest_regressor_results &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;as_tibble&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span style=&#34;color:#a6e22e&#34;&gt;summarise&lt;/span&gt;(
        MSE_no_regressors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mean&lt;/span&gt;((Production &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; forecast)^2),
        MSE_consumption_regressor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mean&lt;/span&gt;((Production &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; forecast_consumption)^2)
    )

&lt;span style=&#34;color:#75715e&#34;&gt;## # A tibble: 1 x 2&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##   MSE_no_regressors MSE_consumption_regressor&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##               &amp;lt;dbl&amp;gt;                     &amp;lt;dbl&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;## 1              1.20                      1.01&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The MSE is lower in the backtest including consumption as a regressor,
so we probably made the right choice :).&lt;/p&gt;
&lt;h1 id=&#34;key-points&#34;&gt;
  Key Points
  &lt;a class=&#34;heading-link&#34; href=&#34;#key-points&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In forecasting, including features are not guaranteed to improve
model performance, so think carefully before you do.&lt;/li&gt;
&lt;li&gt;Feature selection in forecasting is a trade off between selection
performance and computational cost.&lt;/li&gt;
&lt;li&gt;A way to select features in forecasts that directly measures
predictive performance, is to run a backtest of your forecast
without regressors, then fit a linear regression model to learn
actuals given your forecasts and the regressor. Select the regressor
if the p-value is low.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MLOps tools for developing data pipelines</title>
      <link>https://jackbakerds.com/posts/mlops-for-data-pipelines/</link>
      <pubDate>Sun, 06 Jun 2021 17:06:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/mlops-for-data-pipelines/</guid>
      <description>&lt;p&gt;Designing robust data pipelines can be one of the slowest processes in data science development. It can also require maintenance of complex infrastructure. In this post we outline some useful MLOps tools that help with this process, and some good options for small teams.&lt;/p&gt;
&lt;h2 id=&#34;orchestration&#34;&gt;
  Orchestration
  &lt;a class=&#34;heading-link&#34; href=&#34;#orchestration&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Orchestration is the bread and butter of designing data pipelines. Orchestration tools configure how your data pipelines will run in production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What will trigger a data pipeline to begin (scheduled or based off e.g. data arriving).&lt;/li&gt;
&lt;li&gt;The order of your data pipeline and which processes trigger another.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Orchestration engines work using a DAG or directed acyclic graph. This means that each process will be a node in your graph, with arrows going from one node to another. The arrows determine which processes should follow another. This can be visualised graphically quite nicely, like in the picture below.&lt;/p&gt;

&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/mlops-data-pipeline/Data%20pipeline%20example.png&#34; alt=&#34;Example of a data pipeline DAG&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Example of a data pipeline DAG
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;There are many open source orchestration tools. The classic tools are &lt;a href=&#34;https://github.com/apache/airflow&#34;&gt;airflow&lt;/a&gt; and &lt;a href=&#34;https://github.com/spotify/luigi&#34;&gt;luigi&lt;/a&gt;. Some newer orchestrators have been released since then, that can offer some slightly more modern features, though at the cost of a less developed community. These include: &lt;a href=&#34;https://github.com/argoproj/argo-workflows&#34;&gt;argo&lt;/a&gt; (has to be productionised on kubernetes) and &lt;a href=&#34;https://github.com/argoproj/argo-workflows&#34;&gt;dagster&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One problem with open source orchestration tools, is they need to be productionised and maintained on particular infrastructure (e.g. in the case of argo, this would be a kubernetes cluster). This can be a significant overhead for small data teams.&lt;/p&gt;
&lt;p&gt;For smaller data teams an alternative option is out-of-the-box solutions. One popular option for this is to use &lt;a href=&#34;https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&amp;amp;step-functions.sort-order=desc&#34;&gt;AWS Step functions&lt;/a&gt;, and &lt;a href=&#34;https://aws.amazon.com/batch/&#34;&gt;AWS batch&lt;/a&gt;; or similar services offered by other cloud providers. Because step functions are serverless, and AWS batch only boots up the EC2 when a job is running, these options can be cost effective. Alternatively, most out of the box, commercial data science platforms will have orchestrators built in.&lt;/p&gt;
&lt;h2 id=&#34;ml-reproducibility-and-deployment&#34;&gt;
  ML Reproducibility and Deployment
  &lt;a class=&#34;heading-link&#34; href=&#34;#ml-reproducibility-and-deployment&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The ML lifecycle is starting to become more defined, with experimentation performed on a local machine or cheap virtual machine in the cloud; the code is then refactored and deployed if the experimentation is successful.&lt;/p&gt;
&lt;p&gt;The problem is that the refactoring and deployment step can take a long time if processes are not put in place.&lt;/p&gt;
&lt;p&gt;This next set of tools I find particularly interesting. They aim to bridge the gap between experimentation and deployment and make it less painful. They do this by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implementing pipelining tools that make it easier to deploy code on orchestrators.&lt;/li&gt;
&lt;li&gt;Model and data versioning and reproducibility to help keep track of experiments and easily deploy new models.&lt;/li&gt;
&lt;li&gt;Having data access and credentials tools which make it easier to change where data is kept (for example if data is moved from local storage, to S3, to a database).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tools for this include: &lt;a href=&#34;https://metaflow.org/&#34;&gt;metaflow&lt;/a&gt; built by the Netflix team and designed to work seamlessly with AWS; &lt;a href=&#34;https://mlflow.org/&#34;&gt;mlflow&lt;/a&gt; a popular tool for model management; &lt;a href=&#34;https://github.com/quantumblacklabs/kedro&#34;&gt;kedro&lt;/a&gt; a similar tool to metaflow but not as constrained to AWS infrastructure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Upweighting your recent observations in regression and classification</title>
      <link>https://jackbakerds.com/posts/upweight-recent-observations-regression-classification/</link>
      <pubDate>Sun, 06 Jun 2021 15:59:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/upweight-recent-observations-regression-classification/</guid>
      <description>&lt;p&gt;In any regression or classification problem where observations have a time element, old patterns can become stale. For this reason, I&amp;rsquo;m often asking myself &amp;ndash; how do I upweight my most recent observations? In this post I explain how to do this.&lt;/p&gt;
&lt;p&gt;All the code in this tutorial is available as a &lt;a href=&#34;https://github.com/jackcbaker/blog-notebooks/blob/main/regression-forgetting.ipynb&#34;&gt;jupyter notebook&lt;/a&gt; on my &lt;a href=&#34;https://github.com/jackcbaker/&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-1-fetch-the-data&#34;&gt;
  Step 1: Fetch the data
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-1-fetch-the-data&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;For this post I&amp;rsquo;ll be using price data from the &lt;a href=&#34;https://www.kaggle.com/camnugent/sandp500&#34;&gt;S&amp;amp;P500 available via Kaggle&amp;rsquo;s&lt;/a&gt; great dataset library. This dataset is released under the &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;creative commons 0 license&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A common problem in financial investments is to ensure you have a &lt;em&gt;balanced portfolio&lt;/em&gt;. If your portfolio is unbalanced it means that if one stock starts to perform poorly, all of them do. This means any portfolio risks can have a massive effect on value.&lt;/p&gt;
&lt;p&gt;The ideal scenario is that if any stock started to decrease, this would be &lt;em&gt;balanced&lt;/em&gt; by an increase in another one: a balanced portfolio.&lt;/p&gt;
&lt;p&gt;So we can just calculate the correlation coefficient between all the stocks in our portfolio and be done right? Not quite. These relationships tend to change through time, which is something we need to account for.&lt;/p&gt;
&lt;p&gt;In this post we&amp;rsquo;ll use a regression model where recent observations are given more weight to model the relationship between two stocks. This ensures when deciding how balanced the stocks are we are taking into account that the relationship is likely to change over time.&lt;/p&gt;
&lt;p&gt;The tutorial can be applied to any regression or classification problem where you suspect observations are likely to &amp;lsquo;get stale&amp;rsquo; or where relationships may change over time.&lt;/p&gt;
&lt;p&gt;For this tutorial we&amp;rsquo;re going to compare the closing share price of American Express (AXP) to Apple&amp;rsquo;s (AAPL). First let&amp;rsquo;s load the dataset after downloading it from Kaggle&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#75715e&#34;&gt;# for plotting&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns
&lt;span style=&#34;color:#75715e&#34;&gt;# We&amp;#39;ll need this later...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.linear_model &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LinearRegression

&lt;span style=&#34;color:#75715e&#34;&gt;# Load in the full dataset rather than individual stocks&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/all_stocks_5yr.csv&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Subset to just Nike and Apple stocks&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loc[snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isin([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;])]
&lt;span style=&#34;color:#75715e&#34;&gt;# Just take the columns we need&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;]]
&lt;span style=&#34;color:#75715e&#34;&gt;# Set dates to be datetime objects&lt;/span&gt;
snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_datetime(snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])
&lt;span style=&#34;color:#75715e&#34;&gt;# Plot the data&lt;/span&gt;
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lineplot(data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df, x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;, hue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Reshape the data to wide format&lt;/span&gt;
snp_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pivot(index&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;, columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;, values&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;close&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/upweight-recent-observations/stock_plot.png&#34; alt=&#34;Plot of the closing prices of Apple stock vs. American Express&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Plot of the closing prices of Apple vs. American Express.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;We can see from the plot above that initially there is quite a strong relationship between Apple and American Express prices. This is weakened when the American Express price starts to fall. But after this the relationship appears to get stronger again. This suggests a model that accounts for the fact that relationships change through time might be better. It also suggests that most of the time these stocks are imbalanced: they tend to move together.&lt;/p&gt;
&lt;h2 id=&#34;step-2-adding-weights&#34;&gt;
  Step 2: Adding weights
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-2-adding-weights&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Now we need to decide how to weight the observations. There are lots of options here, which is a bit confusing.&lt;/p&gt;
&lt;p&gt;I tend to use the same weights as used in &lt;a href=&#34;https://otexts.com/fpp3/ses.html&#34;&gt;exponential smoothing models&lt;/a&gt;. Why? Exponential smoothing is a popular, simple forecasting method that has been around for over 60 years! It&amp;rsquo;s tried and tested, and has not gone away.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s set \(T\) to be the time of the most recent observation, \(t\) to be the time of the observation we&amp;rsquo;re interested in, and \(\gamma\) to be a hyperparameter we pick that&amp;rsquo;s between 0 and 1. Then I set my weights \(w\) to be&lt;/p&gt;
&lt;p&gt;\begin{equation}
w = \gamma^{[T-t]}.
\end{equation}&lt;/p&gt;
&lt;p&gt;What does this mean? Suppose we set \(\gamma = 0.95\). Then if my observation is made at the most recent time point, its weight will be 1. If it&amp;rsquo;s made at the second most recent time point, its weight will be 0.95. If it&amp;rsquo;s made at the 10th most recent time point, its weight will be \(\gamma^{10} = 0.95^{10} \approx 0.6\). Essentially our weight smoothly decreases to nothing as the observation gets older and older.&lt;/p&gt;
&lt;p&gt;An unfortunate side effect to this is we&amp;rsquo;ve added a hyperparameter to tune. I often just quickly do this by eye looking at the data, but you can also tune this in the normal way using cross-validation or AIC/BIC. For this tutorial, I&amp;rsquo;ll just set it to 0.8.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s add weights to our data now:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set hyperparam&lt;/span&gt;
gamma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Check both stocks go up to the same date&lt;/span&gt;
most_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max()
days_before_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (most_recent_date &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;days
snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gamma &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; days_before_recent_date&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values
snp_df
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;AAPL&lt;/th&gt;
      &lt;th&gt;AXP&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2013-02-08&lt;/td&gt;
      &lt;td&gt;67.8542&lt;/td&gt;
      &lt;td&gt;61.80&lt;/td&gt;
      &lt;td&gt;1.377927e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2013-02-11&lt;/td&gt;
      &lt;td&gt;68.5614&lt;/td&gt;
      &lt;td&gt;61.98&lt;/td&gt;
      &lt;td&gt;2.691264e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2013-02-12&lt;/td&gt;
      &lt;td&gt;66.8428&lt;/td&gt;
      &lt;td&gt;62.20&lt;/td&gt;
      &lt;td&gt;3.364080e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2013-02-13&lt;/td&gt;
      &lt;td&gt;66.7156&lt;/td&gt;
      &lt;td&gt;62.10&lt;/td&gt;
      &lt;td&gt;4.205100e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2013-02-14&lt;/td&gt;
      &lt;td&gt;66.6556&lt;/td&gt;
      &lt;td&gt;62.34&lt;/td&gt;
      &lt;td&gt;5.256375e-177&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1254&lt;/th&gt;
      &lt;td&gt;2018-02-01&lt;/td&gt;
      &lt;td&gt;167.7800&lt;/td&gt;
      &lt;td&gt;100.00&lt;/td&gt;
      &lt;td&gt;2.621440e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1255&lt;/th&gt;
      &lt;td&gt;2018-02-02&lt;/td&gt;
      &lt;td&gt;160.5000&lt;/td&gt;
      &lt;td&gt;96.68&lt;/td&gt;
      &lt;td&gt;3.276800e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1256&lt;/th&gt;
      &lt;td&gt;2018-02-05&lt;/td&gt;
      &lt;td&gt;156.4900&lt;/td&gt;
      &lt;td&gt;92.01&lt;/td&gt;
      &lt;td&gt;6.400000e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1257&lt;/th&gt;
      &lt;td&gt;2018-02-06&lt;/td&gt;
      &lt;td&gt;163.0300&lt;/td&gt;
      &lt;td&gt;94.18&lt;/td&gt;
      &lt;td&gt;8.000000e-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1258&lt;/th&gt;
      &lt;td&gt;2018-02-07&lt;/td&gt;
      &lt;td&gt;159.5400&lt;/td&gt;
      &lt;td&gt;93.61&lt;/td&gt;
      &lt;td&gt;1.000000e+00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1259 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;step-3-fit-your-model&#34;&gt;
  Step 3: Fit your model
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-3-fit-your-model&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;You might think that fitting our model will become a challenge now we&amp;rsquo;ve added weights. It&amp;rsquo;s actually dead easy.&lt;/p&gt;
&lt;p&gt;Most regression and classification algorithms allow you to provide a dataset weight: for tree based methods (sklearn random forest, xgboost, lightgbm), you just set the &lt;code&gt;sample_weight&lt;/code&gt; in the &lt;code&gt;fit&lt;/code&gt; function; for linear regression R&amp;rsquo;s &lt;code&gt;lm&lt;/code&gt; function has a &lt;code&gt;weights&lt;/code&gt; argument, sklearn&amp;rsquo;s &lt;code&gt;LinearRegression&lt;/code&gt; has a &lt;code&gt;sample_weight&lt;/code&gt; argument in the &lt;code&gt;fit&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If your algorithm does not allow you to set a weight, you can borrow from the class imbalance techniques and &lt;a href=&#34;https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/&#34;&gt;oversample/undersample&lt;/a&gt; your observations based on your weights.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s fit our model using sklearn&amp;rsquo;s linear regression:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;weighted_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LinearRegression()
weighted_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weighted_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;], 
    sample_weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#75715e&#34;&gt;# Return the R^2 score for our model&lt;/span&gt;
r2_weighted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weighted_results&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;], 
    sample_weight&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;weight&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;R^2 score for the weighted model is {r2_weighted}&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Linear coefficient of relationship of Apple to American Express is {weighted_model.coef_[0]}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;R^2 score for the weighted model is 0.740848229773393
Linear coefficient of relationship of Apple to American Express is 0.4810444526330664
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s compare this to an unweighted model:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;unweighted_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LinearRegression()
unweighted_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; unweighted_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#75715e&#34;&gt;# Return the R^2 score for our model&lt;/span&gt;
r2_unweighted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; unweighted_results&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;score(
    X&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AAPL&amp;#39;&lt;/span&gt;]],
    y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;snp_df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;AXP&amp;#39;&lt;/span&gt;]
)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;R^2 score for the unweighted model is {r2_unweighted}&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Linear coefficient of relationship of Apple to American Express is {unweighted_model.coef_[0]}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;R^2 score for the unweighted model is 0.1161401914047755
Linear coefficient of relationship of Apple to American Express is 0.12149913600503894
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we&amp;rsquo;ve improved our in-sample fit significantly by using the weighted model. The coefficient is larger for the weighted model. This suggests that while previously, these stocks have not been that related, recently the stocks have become less balanced. This makes sense from what we saw when we plotted the two timeseries. This would be an increasing risk for our portfolio.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A python equivalent for R markdown</title>
      <link>https://jackbakerds.com/posts/python-equivalent-rmarkdown/</link>
      <pubDate>Tue, 04 May 2021 20:20:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/python-equivalent-rmarkdown/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R markdown&lt;/a&gt; is a powerful tool for sharing insights with stakeholders. You can write snippets of R code that generate plots. This can then be compiled to a HTML or pdf file that you can share with non-technical stakeholders.&lt;/p&gt;
&lt;p&gt;This is not as straightforward in python. Yes, Jupyter notebooks are a great way of sharing analysis with other developers. But compiling to HTML/pdf, with code snippets removed, that looks nice enough for a non-technical stakeholder, I&amp;rsquo;ve found clunky using Jupyter notebooks. R markdown also has great tools to generate &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/kable.html&#34;&gt;nice looking tables&lt;/a&gt;, not just plots.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve been working with a python heavy team though, so have been trying to figure out how to generate R markdown style documents. In this post I&amp;rsquo;ll outline what I&amp;rsquo;ve been using to generate HTML reports in python, that look nice enough to share with non-technical stakeholders. The process uses a few tools.&lt;/p&gt;
&lt;h2 id=&#34;step-1-embedding-plots-in-html&#34;&gt;
  Step 1: Embedding Plots in HTML
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-1-embedding-plots-in-html&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The first step is to embed plots into a static HTML that you can then share with others. A great tool for this is &lt;a href=&#34;https://plotly.com/python/&#34;&gt;plotly&lt;/a&gt;. Plotly has a &lt;a href=&#34;https://plotly.com/python-api-reference/generated/plotly.io.to_html.html&#34;&gt;&lt;code&gt;to_html&lt;/code&gt; function&lt;/a&gt; (one of my amazing colleagues found this) which will write the plots as a HTML string, which you can then write to a file.&lt;/p&gt;
&lt;p&gt;Plotly graphs look the part, and they allow the user to hover over the points to see what the values are; I&amp;rsquo;ve found this goes down well with customers.&lt;/p&gt;
&lt;p&gt;Sometimes users need plots they can copy-paste though. In this case I recommend using python&amp;rsquo;s more standard plotting libraries, like &lt;a href=&#34;https://seaborn.pydata.org/&#34;&gt;seaborn&lt;/a&gt; or &lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;. To embed the image into HTML without needing to have a separate image file the HTML references is a bit more involved. But you can do it by encoding the image as base64 and writing directly to your HTML. Just follow the instructions in this &lt;a href=&#34;https://stackoverflow.com/questions/48717794/matplotlib-embed-figures-in-auto-generated-html&#34;&gt;stackoverflow post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-2-setting-the-layout-of-the-html-document&#34;&gt;
  Step 2: Setting the Layout of the HTML Document
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-2-setting-the-layout-of-the-html-document&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Great, now you can embed plots in HTML, and actually if you turn off the &lt;code&gt;full_html&lt;/code&gt; option in plotly&amp;rsquo;s &lt;code&gt;to_html&lt;/code&gt; function, you can write as many plotly plots as you like to a html file by just appending the strings.&lt;/p&gt;
&lt;p&gt;But what about layout, commentary, and tables of data? This is where I use python&amp;rsquo;s HTML templating libraries &amp;ndash; which allow you to use python to generate static HTML files from templates. This technique is powerful, and used in python backend development libraries like &lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/&#34;&gt;flask&lt;/a&gt; and &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;django&lt;/a&gt;. It&amp;rsquo;s quick to learn, but does require some knowledge of HTML.&lt;/p&gt;
&lt;p&gt;I use the &lt;a href=&#34;https://jinja.palletsprojects.com/en/2.11.x/&#34;&gt;Jinja&lt;/a&gt; library to generate my HTML reports (it&amp;rsquo;s one of the most popular HTML templating libraries in python).&lt;/p&gt;
&lt;h2 id=&#34;step-3-making-the-report-look-nice&#34;&gt;
  Step 3: Making the Report Look Nice
  &lt;a class=&#34;heading-link&#34; href=&#34;#step-3-making-the-report-look-nice&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;For anyone that&amp;rsquo;s worked with HTML before you&amp;rsquo;ll know it takes a long time to make anything which looks presentable.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t want to spend much time on this because I wanted to generate the report as quick as possible. So I used the &lt;a href=&#34;https://getbootstrap.com/&#34;&gt;bootstrap&lt;/a&gt; CSS library. This tool allows you to make your report look nice in a short amount of time (all I tend to do is wrap everything in a &lt;code&gt;&amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;&lt;/code&gt; and maybe add some padding).&lt;/p&gt;
&lt;p&gt;Bootstrap is basically a set of prebuilt HTML classes you can use to format your HTML. For example to make an HTML table look nice in bootstrap it&amp;rsquo;s as simple as &lt;code&gt;&amp;lt;table class=&amp;quot;table&amp;quot;&amp;gt;&lt;/code&gt; (tables look quite ugly in standard HTML). You can add padding at the top of a title element by adding &lt;code&gt;&amp;lt;h1 class=&amp;quot;pt-1&amp;quot;&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;thoughts&#34;&gt;
  Thoughts
  &lt;a class=&#34;heading-link&#34; href=&#34;#thoughts&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;As you can see, this is a fiddlier process than with R markdown (if anyone has a better way, please get in touch!). But the tools are useful to learn, especially Jinja and bootstrap which are standard tools for web development. Once you&amp;rsquo;ve learnt the libraries, and have some pre-made templates ready to go, the process gets quite quick.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three ways to speed up developing your data science solutions</title>
      <link>https://jackbakerds.com/posts/speed-up-data-science-development/</link>
      <pubDate>Mon, 26 Apr 2021 21:00:00 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/speed-up-data-science-development/</guid>
      <description>&lt;p&gt;Your customers want results and quickly. This can be stressful as a data scientist: solutions are often experimental and results not guaranteed; you need time to think about the problem.&lt;/p&gt;
&lt;p&gt;In this post I&amp;rsquo;ll share three ways I use to develop data science solutions faster.&lt;/p&gt;
&lt;p&gt;I find there are a few time sinks when developing data science solutions: going down a rabbit hole or dead end, productionising, checking your solution actually beats the current business process, and ensuring your code is free of bugs. How do I try to avoid these pitfalls?&lt;/p&gt;
&lt;h2 id=&#34;1-start-as-simple-as-possible&#34;&gt;
  1. Start as simple as possible
  &lt;a class=&#34;heading-link&#34; href=&#34;#1-start-as-simple-as-possible&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;When I start a new problem, I implement the simplest possible solution I can think of. Once this is developed and tested, I iteratively improve on it. What this gets you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A baseline you can compare all your models against. This helps stop you going down a rabbit hole: I talk about this more later.&lt;/li&gt;
&lt;li&gt;A quick, quality start &amp;ndash; I regularly find a baseline beats a more complex model.&lt;/li&gt;
&lt;li&gt;A chance to develop the pipeline without worrying about intricate model headaches. I find this makes a solution easier to productionise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This also applies to iterations on your baseline. Break these down so they&amp;rsquo;re as small as possible. If you follow the third point this becomes powerful.&lt;/p&gt;
&lt;h2 id=&#34;2-modularise&#34;&gt;
  2. Modularise
  &lt;a class=&#34;heading-link&#34; href=&#34;#2-modularise&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Often a data science solution consists of lots of mini problems you need to solve. I recommend you break down your problem as much as possible into components. What this gets you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can write each modular part as separate code modules. Set these to take relatively general input and output, and your code will be much easier to productionise: all you need to write are interfaces to your particular infrastructure (which is reusable code).&lt;/li&gt;
&lt;li&gt;Your code will be easier to test, which helps ensure it&amp;rsquo;s bug free. For example using the technique I talk about &lt;a href=&#34;https://jackcbaker.github.io/posts/check-data-science-pipeline-working/&#34;&gt;in this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you follow the first point, your code will be reusable for future solutions.&lt;/li&gt;
&lt;li&gt;Your baseline becomes even simpler :).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, say you&amp;rsquo;re developing a churn model, you might have: one or two modules handling data transformation, a module that handles the model build, one that handles serving predictions, and one that handles customer visualisations.&lt;/p&gt;
&lt;h2 id=&#34;3-test-each-iteration-against-your-baseline-the-current-business-process-and-your-best-model-so-far&#34;&gt;
  3. Test each iteration against your baseline, the current business process and your best model so far
  &lt;a class=&#34;heading-link&#34; href=&#34;#3-test-each-iteration-against-your-baseline-the-current-business-process-and-your-best-model-so-far&#34;&gt;
    &lt;i class=&#34;fa fa-link&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;If you follow what I mentioned earlier and keep your model iterations small, this can really keep you on track:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You catch rabbit holes early by seeing that an iteration made no improvement.&lt;/li&gt;
&lt;li&gt;You can hone in on which module needs most improvement.&lt;/li&gt;
&lt;li&gt;You can fail fast &amp;ndash; if something isn&amp;rsquo;t working, you&amp;rsquo;ll know it quickly and can manage customer expectations.&lt;/li&gt;
&lt;li&gt;You have regular tangible results to share with customers.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to move your data science solution into a hands off state</title>
      <link>https://jackbakerds.com/posts/data-science-solution-to-hands-off-state/</link>
      <pubDate>Sat, 17 Apr 2021 15:36:34 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/data-science-solution-to-hands-off-state/</guid>
      <description>
&lt;figure &gt;
    &lt;style scoped&gt;
        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    &lt;/style&gt;
    
        &lt;img src=&#34;https://jackbakerds.com/post_images/hands-off/relaxed.jpg&#34; alt=&#34;A person relaxing next to a lake&#34; class=&#34;center&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Photo by 
        &lt;a href=&#34;https://unsplash.com/@simonmigaj?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt; 
            S Migaj on Unsplash.
        &lt;/a&gt; 
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

&lt;p&gt;Once you&amp;rsquo;ve developed a data science solution it&amp;rsquo;s easy to tinker with it indefinitely: running it manually, making small changes to the output, iterating on the model. This is not ideal - once you have a solution you&amp;rsquo;re happy with ideally you should be able to leave it running with minimal input from you, unless something falls over.&lt;/p&gt;
&lt;p&gt;Whether you&amp;rsquo;re handing over to a support team, or will be supporting the tool yourself, this post gives some tips when you&amp;rsquo;re trying to move your solution into a hands-off state - or putting it into production.&lt;/p&gt;
&lt;p&gt;Firstly you want to be logging. Logging is where output from your code is printed to a file so you (or the support team) can refer to it later. To move your code into a hands-off state you want to have logs that save to a file, so you can inspect these later if something goes wrong with the tool.&lt;/p&gt;
&lt;p&gt;What should you be logging? Any obvious things that might go wrong with the tool, such as data validation input, or the tool not being able to connect to a web service, etc. should be flagged as a warning or error and logged. Another good rule of thumb is to flag any &amp;lsquo;transaction&amp;rsquo;. What I mean by this is things like reading files, connecting to a database or calling an API. These are all things that can fail and help serve as a rough guide for where your code failed and what likely went wrong. It&amp;rsquo;s a good idea to have a summary at the end of a log indicating if the tool finished successfully, and if it failed, what went wrong. A log summary is really appreciated by support teams.&lt;/p&gt;
&lt;p&gt;Another useful tool is alerting. This is where your tool notifies you (or the support team) when something went wrong or failed. One of the most effective ways of doing this is to get your tool to message you on slack or MS teams when something&amp;rsquo;s gone wrong. This is very easy to do using webhooks. You can also use email for this e.g. using AWS SNS. It&amp;rsquo;s a good idea to notify you when something&amp;rsquo;s gone wrong, as well as when the tool is started and finished; then absence of these alerts will also tell you something is wrong.&lt;/p&gt;
&lt;p&gt;Triage tables are invaluable, especially if you are handing over to a support team or someone is keeping an eye on the tool while you&amp;rsquo;re away. Triage tables list the most common things that can go wrong with the tool, as well as the team that should be contacted to resolve this issue. For example if your tool relies on a data feed that&amp;rsquo;s managed by another team; if this breaks you can request support to contact that team directly. That way the issue can be resolved without you acting as an intermediary which wastes time for everyone. The act of doing this also forces you to think carefully about your tool and what error handling should be in place, to make sure it&amp;rsquo;s clear from the logs what went wrong.&lt;/p&gt;
&lt;p&gt;Finally, you have to make sure you have a business&amp;rsquo;s best interests at heart. It&amp;rsquo;s easy as a data scientist to make constant iterations to a model, or to agree to every request a customer asks for. But at the end of the day, your job is to provide as much value to a business as possible. You want a model to be effective, but you don&amp;rsquo;t need it to be perfect, after a point you can probably provide more value working on a different solution. Likewise, it&amp;rsquo;s imperative to listen to what the customer wants, but after a time of ensuring you have met customer requirements, you might be able to provide better value by building the customer a different tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to tell your data science pipeline is actually working?</title>
      <link>https://jackbakerds.com/posts/check-data-science-pipeline-working/</link>
      <pubDate>Sun, 11 Apr 2021 15:36:34 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/check-data-science-pipeline-working/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been there - studying model output for hours to check a model is working. There&amp;rsquo;s some better ways than this. For example designing backtests to check a model does better than a baseline. The trouble with these methods is they don&amp;rsquo;t categorically show that a model doing exactly as it is designed to do; they&amp;rsquo;re also not automated, or (as with a backtest) are expensive to run; and sometimes they&amp;rsquo;re harder to design than the model itself! Tests that are not automated mean as soon as you make changes you need to do another lengthy check or risk the model being invalid. Other options are unit and integration tests, which are automated, but these don&amp;rsquo;t explicitly check the model is working correctly.&lt;/p&gt;
&lt;p&gt;The way I solve this problem is test datasets. We often spend a lot of time as data scientists waiting for data to arrive. Why not create a test dataset during that time (it only takes about an hour for simple pipelines, which is a lot less than typical data lead time). This test dataset should: be modelled off the schema of your source data; have output that, once processed through your modelling pipeline, is known exactly. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you have a regression pipeline, you could generate random linearly related data and check held-out predictions match your actuals within a desired level of tolerance.&lt;/li&gt;
&lt;li&gt;If you have a forecasting pipeline, you could check it can predict on timeseries data generated from an ARIMA process within a desired level of tolerance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This does not replace the function of a backtest: it is not checking if the model actually works well on the problem at hand. But what it does is check that your modelling pipeline is doing exactly what it&amp;rsquo;s designed to do. This is especially important when you have quite a complex pipeline which is doing multiple data transformations before applying the model. It&amp;rsquo;s easy to make a mistake with these transformations and not notice. The test can also be automated, and is cheap to run (if you keep your test dataset small), so you get a free integration test on your full pipeline. That way if you make any changes you can just run the test again to check things are still working as expected. You can also add some tests in between different stages of the pipeline to get integration tests on different parts of the pipeline for free.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to include libraries in your project while also editing them</title>
      <link>https://jackbakerds.com/posts/include-libraries-project-editing/</link>
      <pubDate>Tue, 06 Apr 2021 18:29:47 +0100</pubDate>
      
      <guid>https://jackbakerds.com/posts/include-libraries-project-editing/</guid>
      <description>&lt;p&gt;I came across this issue the other day: I wanted to include a library in a project, but to make it work in my code I would have to edit it. I then needed to write those changes back to the original library, with minimal faff. This post explains the best way I found of doing this.&lt;/p&gt;
&lt;p&gt;An obvious solution is to include the whole codebase of the library into your project. This will allow you to make the required changes. This can be quite messy though: your repo can become very large if you&amp;rsquo;re working with a big library, and it&amp;rsquo;s a pain to push your changes back to the original project.&lt;/p&gt;
&lt;p&gt;I found a better way of doing this: &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;&lt;code&gt;git submodule&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;git submodule&lt;/code&gt; allows you to manage additional repos within your project. Using it, the submodule repo is contained in a directory in your own project. By going into the directory that contains that repo you can make changes to that module using the standard git workflow, and push them back to the original library.&lt;/p&gt;
&lt;p&gt;When you push your own project repo, it will just contain a reference to the libraries repo location, and a commit-id. This keeps it small and concise. A great tutorial on how &lt;code&gt;git submodule&lt;/code&gt; works is &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a big push for reusability in code at the moment. This is a great way to keep your code as small reusable components rather than a big monolith.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>